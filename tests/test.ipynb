{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.append(\"../rag\")\n",
    "from retrieval import Retrieval\n",
    "# rag_path = os.path.abspath(os.path.join('.', 'context\\knowledge_base\\A Clockwork Orange (novel)\\chunk_10.txt'))\n",
    "# print(f\"rag_path: {rag_path}\")  # Check if this is the correct folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\krazy\\AppData\\Local\\Temp\\ipykernel_25604\\2069103489.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  retrieval = Retrieval(\"..\\context\\knowledge_base\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transformer model and tokenizer from transformers library: avsolatorio/NoInstruct-small-Embedding-v0\n",
      "Please wait...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieval = Retrieval(\"..\\context\\knowledge_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krazy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krazy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('A, or a, is the first letter and the first vowel letter of the Latin alphabet, used in the modern English alphabet, and others worldwide. Its name in English is \"a\" (pronounced ), plural \"aes\".',\n",
       " 'A')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval.get_context(\"What is the first principle of animation?\", num_contexts=1, use_sparse_retrieval=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
