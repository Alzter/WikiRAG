{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf08944f48a94b02918b86125339f3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/756 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\gpu_tf\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alex\\.cache\\huggingface\\hub\\models--avsolatorio--NoInstruct-small-Embedding-v0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6965cc6bde9c47fda78540139a58daa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f888dae6ab0346e09ba6928ba8eedbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0365b1484fcd453b8fbf2d6d20bee1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a0de54224b48cfa697dd2c59ae8feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd0a29c011b4141a42aad59f3604e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source: https://huggingface.co/avsolatorio/NoInstruct-small-Embedding-v0\n",
    "\n",
    "from typing import Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"avsolatorio/NoInstruct-small-Embedding-v0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avsolatorio/NoInstruct-small-Embedding-v0\")\n",
    "\n",
    "\n",
    "# Source: https://huggingface.co/avsolatorio/NoInstruct-small-Embedding-v0\n",
    "def get_embedding(text: Union[str, list[str]], mode: str = \"sentence\"):\n",
    "    model.eval()\n",
    "\n",
    "    assert mode in (\"query\", \"sentence\"), f\"mode={mode} was passed but only `query` and `sentence` are the supported modes.\"\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "\n",
    "    inp = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inp)\n",
    "\n",
    "    # The model is optimized to use the mean pooling for queries,\n",
    "    # while the sentence / document embedding uses the [CLS] representation.\n",
    "\n",
    "    if mode == \"query\":\n",
    "        vectors = output.last_hidden_state * inp[\"attention_mask\"].unsqueeze(2)\n",
    "        vectors = vectors.sum(dim=1) / inp[\"attention_mask\"].sum(dim=-1).view(-1, 1)\n",
    "    else:\n",
    "        vectors = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.72777176 0.68083483]\n",
      " [0.72777176 1.         0.7170707 ]\n",
      " [0.68083483 0.7170707  0.9999999 ]]\n",
      "[0.6533681  0.5988234  0.72371066]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Illustration of the REaLTabFormer model. The left block shows the non-relational tabular data model using GPT-2 with a causal LM head. In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model. The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table. The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.\",\n",
    "    \"Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\",\n",
    "    \"As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands. However, little is known about the skills that workers need to adapt to these changes\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = get_embedding(texts, mode=\"sentence\")\n",
    "\n",
    "# Compute cosine-similarity for each pair of sentences\n",
    "scores = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)\n",
    "print(scores.cpu().numpy())\n",
    "\n",
    "# Test the retrieval performance.\n",
    "query = get_embedding(\"Which sentence talks about concept on jobs?\", mode=\"query\")\n",
    "\n",
    "scores = F.cosine_similarity(query, embeddings, dim=-1)\n",
    "print(scores.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
