{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump raw Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump saved to dumps/wikipedia_dump_file.bz2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_wikipedia(dump_url, download_subset, \n",
    "                       dump_file_path='dumps/wikipedia_dump_file.bz2'):\n",
    "    \"\"\"\n",
    "    Downloads the full Wikipedia dump or a 5 MB subset.\n",
    "\n",
    "    Args:\n",
    "        dump_url (str): The URL of the Wikipedia dump file to be downloaded.\n",
    "        download_subset (bool): If true, only downloads a 5 MB subset of the Wikipedia dump.\n",
    "        dump_file_path (str, optional): Path where the downloaded file will be\n",
    "        saved (default: 'wikipedia_dump_file.bz2').\n",
    "    \n",
    "    Returns:\n",
    "        path (str): The path to the saved dump file.\n",
    "    \"\"\"\n",
    "\n",
    "    dump_folder = os.path.split(dump_file_path)[0]\n",
    "    if not os.path.exists(dump_folder): os.makedirs(dump_folder)\n",
    "\n",
    "    # Stream the file download based on user's choice (0 = full, 1 = subset)\n",
    "    with requests.get(dump_url, stream=True) as r, open(dump_file_path, 'wb') as f:\n",
    "        r.raise_for_status()  # Raise an error for bad responses\n",
    "        \n",
    "        # Download 5 MB subset\n",
    "        if download_subset:\n",
    "            downloaded_size = 0\n",
    "            for chunk in r.iter_content(1024):  # Download data in chunks of 1 KB\n",
    "                if chunk:\n",
    "                    f.write(chunk)  # Write the chunk to the file\n",
    "                    downloaded_size += len(chunk)  # Increase size counter\n",
    "                    if downloaded_size >= 5_000_000:  # Stop after 5 MB\n",
    "                        break\n",
    "        \n",
    "        # Download the full dump\n",
    "        else:\n",
    "            for chunk in r.iter_content(1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded full dump.\")\n",
    "    \n",
    "    print(f\"Dump saved to {dump_file_path}\")\n",
    "    return dump_file_path\n",
    "\n",
    "user_choice = bool(input(\"Enter 1 to download a 5MB subset or 0 for the full dump: \"))\n",
    "dump_url = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2'\n",
    "dump = download_wikipedia(dump_url, user_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw text from Wikipedia Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../wiki_extractor\")\n",
    "import WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded 0 templates in 0.0s\n",
      "INFO: Starting page extraction from dumps/wikipedia_dump_file.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Finished 7-process extraction of 250 articles in 3.4s (74.0 art/s)\n",
      "INFO: total of page: 250, total of articl page: 250; total of used articl page: 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.  Extracted files are saved in wikipedia_extracted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def extract_wikipedia_dump(dump_file_path, output_dir='wikipedia_extracted', is_subset=True, use_local_wikiextractor = True):\n",
    "    \"\"\"\n",
    "    Extracts plain text from the Wikipedia dump using WikiExtractor.\n",
    "\n",
    "    Args:\n",
    "        dump_file_path (str): The path to the downloaded Wikipedia dump file.\n",
    "        output_dir (str, optional): The directory where the extracted text will be saved.\n",
    "                                    Defaults to 'wikipedia_extracted'.\n",
    "        is_subset (bool, optional): If True, assume the dump file is a small subset (e.g., 5MB).\n",
    "                                    If False, process the full dump. Defaults to True.\n",
    "        use_local_wikiextractor (bool, optional):\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #  Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if use_local_wikiextractor:\n",
    "        if is_subset:\n",
    "            WikiExtractor.main(input=dump_file_path, json=True, no_templates=True, output=output_dir, bytes=\"5M\")\n",
    "        else:\n",
    "            WikiExtractor.main(input=dump_file_path, json=True, no_templates=True, output=output_dir)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Use '--bytes' for smaller subsets\n",
    "        extractor_command = ['wikiextractor', '--json', '--no-templates', '-o', output_dir, dump_file_path]\n",
    "\n",
    "        if is_subset:\n",
    "           extractor_command.insert(1, '--bytes')\n",
    "           extractor_command.insert(2, '5M')  #5  MB\n",
    "\n",
    "        print(extractor_command)\n",
    "\n",
    "        # Run wikiextractor via subprocess\n",
    "        subprocess.run(extractor_command, check=True)\n",
    "\n",
    "    print(f\"Extraction completed.  Extracted files are saved in {output_dir}\")\n",
    "\n",
    "#Test\n",
    "extract_wikipedia_dump(dump, is_subset=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Wikipedia Raw Text Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk articles by sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.43.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Run the device on GPU only if NVIDIA CUDA drivers are installed.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# load model and tokenizer\n",
    "\n",
    "model_name = \"jinaai/jina-embeddings-v2-base-en\" #\"avsolatorio/NoInstruct-small-Embedding-v0\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, device_map='cuda', quantization_config = quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, device_map='cuda', quantization_config = quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483648"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        input_text: The text snippet to split into sentences.\n",
    "        param tokenizer: The tokenizer to use.\n",
    "    \n",
    "    Returns:\n",
    "        chunks (list): The list of text chunks.\n",
    "        span_annotations (list): The token span for each text chunk.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking by sentences with ``NoInstruct-small-Embedding-v0`` will give us a warning, because the tokens are greater than the model's max token length of ``512``, but the output is identical to ``jina-embeddings-v2-base-en``, despite that model having ``2147483648`` max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Function to read all text content from JSON files in a folder\n",
    "def read_input_texts_from_folder(folder_path):\n",
    "    all_text = \"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Parse each line as JSON and extract the 'text' field\n",
    "                        data = json.loads(line)\n",
    "                        text_content = data.get('text', '').strip()  # Strip any leading/trailing whitespace\n",
    "                        if text_content:  # Ensure only non-empty content is added\n",
    "                            all_text += text_content + \"\\n\"\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip lines that are not valid JSON\n",
    "    return all_text\n",
    "\n",
    "# Folder where the Wikipedia data is extracted\n",
    "input_folder_path = 'wikipedia_extracted/'\n",
    "\n",
    "# Reading all the extracted text files from the folder\n",
    "input_text = read_input_texts_from_folder(input_folder_path)\n",
    "\n",
    "# Print the combined extracted text (for debugging purposes)\n",
    "# if input_text.strip():\n",
    "#     print(f\"Extracted Text: {input_text[:1000]}...\")  # Print first 1000 characters to check\n",
    "# else:\n",
    "#     print(\"No text extracted\")\n",
    "\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk and Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_late_chunking = False\n",
    "\n",
    "if use_late_chunking:\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "    model_output = model(**inputs)\n",
    "    embeddings = late_chunking(model_output, [span_annotations])[0]\n",
    "else:\n",
    "    embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compate results of traditional and late chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "# anarchism_embedding = model.encode('Anarchism')\n",
    "\n",
    "# for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "#     print(f'similarity_new(\"Anarchism\", \"{chunk}\"):', cos_sim(anarchism_embedding, new_embedding))\n",
    "#     print(f'similarity_trad(\"Anarchism\", \"{chunk}\"):', cos_sim(anarchism_embedding, trad_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
