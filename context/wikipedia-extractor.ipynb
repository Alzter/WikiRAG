{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump saved to dumps/wikipedia_dump_file.bz2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_wikipedia(dump_url, download_subset, \n",
    "                       dump_file_path='dumps/wikipedia_dump_file.bz2'):\n",
    "    \"\"\"\n",
    "    Downloads the full Wikipedia dump or a 5 MB subset.\n",
    "\n",
    "    Args:\n",
    "        dump_url (str): The URL of the Wikipedia dump file to be downloaded.\n",
    "        download_subset (bool): If true, only downloads a 5 MB subset of the Wikipedia dump.\n",
    "        dump_file_path (str, optional): Path where the downloaded file will be\n",
    "        saved (default: 'wikipedia_dump_file.bz2').\n",
    "    \n",
    "    Returns:\n",
    "        path (str): The path to the saved dump file.\n",
    "    \"\"\"\n",
    "\n",
    "    dump_folder = os.path.split(dump_file_path)[0]\n",
    "    if not os.path.exists(dump_folder): os.makedirs(dump_folder)\n",
    "\n",
    "    # Stream the file download based on user's choice (0 = full, 1 = subset)\n",
    "    with requests.get(dump_url, stream=True) as r, open(dump_file_path, 'wb') as f:\n",
    "        r.raise_for_status()  # Raise an error for bad responses\n",
    "        \n",
    "        # Download 5 MB subset\n",
    "        if download_subset:\n",
    "            downloaded_size = 0\n",
    "            for chunk in r.iter_content(1024):  # Download data in chunks of 1 KB\n",
    "                if chunk:\n",
    "                    f.write(chunk)  # Write the chunk to the file\n",
    "                    downloaded_size += len(chunk)  # Increase size counter\n",
    "                    if downloaded_size >= 5_000_000:  # Stop after 5 MB\n",
    "                        break\n",
    "        \n",
    "        # Download the full dump\n",
    "        else:\n",
    "            for chunk in r.iter_content(1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded full dump.\")\n",
    "    \n",
    "    print(f\"Dump saved to {dump_file_path}\")\n",
    "    return dump_file_path\n",
    "\n",
    "user_choice = bool(input(\"Enter 1 to download a 5MB subset or 0 for the full dump: \"))\n",
    "dump_url = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2'\n",
    "dump = download_wikipedia(dump_url, user_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikiextractor\n",
      "  Downloading wikiextractor-3.0.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading wikiextractor-3.0.6-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.4/46.4 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: wikiextractor\n",
      "Successfully installed wikiextractor-3.0.6\n"
     ]
    }
   ],
   "source": [
    "# %pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../wiki_extractor\")\n",
    "import WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = 'dumps/wikipedia_dump_file.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded 0 templates in 0.0s\n",
      "INFO: Starting page extraction from dumps/wikipedia_dump_file.bz2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using 7 extract processes.\n",
      "INFO: Finished 7-process extraction of 249 articles in 7.7s (32.5 art/s)\n",
      "INFO: total of page: 249, total of articl page: 249; total of used articl page: 249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.  Extracted files are saved in wikipedia_extracted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def extract_wikipedia_dump(dump_file_path, output_dir='wikipedia_extracted', is_subset=True):\n",
    "    \"\"\"\n",
    "    Extracts plain text from the Wikipedia dump using WikiExtractor.\n",
    "\n",
    "    Args:\n",
    "        dump_file_path (str): The path to the downloaded Wikipedia dump file.\n",
    "        output_dir (str, optional): The directory where the extracted text will be saved.\n",
    "                                    Defaults to 'wikipedia_extracted'.\n",
    "        is_subset (bool, optional): If True, assume the dump file is a small subset (e.g., 5MB).\n",
    "                                    If False, process the full dump. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #  Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if is_subset:\n",
    "        WikiExtractor.main(input=dump_file_path, json=True, no_templates=True, output=output_dir, bytes=\"5M\")\n",
    "    else:\n",
    "        WikiExtractor.main(input=dump_file_path, json=True, no_templates=True, output=output_dir)\n",
    "\n",
    "    #  Use '--bytes' for smaller subsets\n",
    "    #extractor_command = ['wikiextractor', '--json', '--no-templates', '-o', output_dir, dump_file_path]\n",
    "\n",
    "    #if is_subset:\n",
    "    #    extractor_command.insert(1, '--bytes')\n",
    "    #    extractor_command.insert(2, '5M')  #5  MB\n",
    "\n",
    "    #print(extractor_command)\n",
    "\n",
    "    #  Run wikiextractor via subprocess\n",
    "    #subprocess.run(extractor_command, check=True)\n",
    "\n",
    "    print(f\"Extraction completed.  Extracted files are saved in {output_dir}\")\n",
    "\n",
    "#Test\n",
    "extract_wikipedia_dump(dump, is_subset=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.43.4\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (2024.2.2)\n",
      "Downloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed transformers-4.43.4\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers==4.43.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"avsolatorio/NoInstruct-small-Embedding-v0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avsolatorio/NoInstruct-small-Embedding-v0\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "#model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        input_text: The text snippet to split into sentences.\n",
    "        param tokenizer: The tokenizer to use.\n",
    "    \n",
    "    Returns:\n",
    "        chunks (list): The list of text chunks.\n",
    "        span_annotations (list): The token span for each text chunk.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Chunks:\n",
      "- \"\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Function to read all text content from JSON files in a folder\n",
    "def read_input_texts_from_folder(folder_path):\n",
    "    all_text = \"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Parse each line as JSON and extract the 'text' field\n",
    "                        data = json.loads(line)\n",
    "                        text_content = data.get('text', '').strip()  # Strip any leading/trailing whitespace\n",
    "                        if text_content:  # Ensure only non-empty content is added\n",
    "                            all_text += text_content + \"\\n\"\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip lines that are not valid JSON\n",
    "    return all_text\n",
    "\n",
    "# Folder where the Wikipedia data is extracted\n",
    "input_folder_path = 'context/wikipedia_extracted/'\n",
    "\n",
    "# Reading all the extracted text files from the folder\n",
    "input_text = read_input_texts_from_folder(input_folder_path)\n",
    "\n",
    "# Print the combined extracted text (for debugging purposes)\n",
    "# if input_text.strip():\n",
    "#     print(f\"Extracted Text: {input_text[:1000]}...\")  # Print first 1000 characters to check\n",
    "# else:\n",
    "#     print(\"No text extracted\")\n",
    "\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print(chunks)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/.cache/huggingface/modules/transformers_modules/jinaai/jina-bert-implementation/f3ec4cf7de7e561007f27c9efc7148b0bd713f81/modeling_bert.py:776: UserWarning: Increasing alibi size from 8192 to 1298636.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "anarchism_embedding = model.encode('Anarchism')\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Anarchism\", \"{chunk}\"):', cos_sim(anarchism_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Anarchism\", \"{chunk}\"):', cos_sim(anarchism_embedding, trad_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
