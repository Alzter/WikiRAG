{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump saved to wikipedia_dump_file.bz2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_wikipedia(dump_url, choice, \n",
    "                       dump_file_path='wikipedia_dump_file.bz2'):\n",
    "    \"\"\"\n",
    "    Downloads the full Wikipedia dump or a 5 MB subset.\n",
    "\n",
    "    Input Parameters:\n",
    "    - dump_url (str): The URL of the Wikipedia dump file to be downloaded.\n",
    "    - choice (int): Enter 0 for the full download or 1 for a 5 MB subset.\n",
    "    - dump_file_path (str, optional): Path where the downloaded file will be\n",
    "      saved (default: 'wikipedia_dump_file.bz2').\n",
    "    \n",
    "    Returns:\n",
    "    - str: The path to the saved dump file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stream the file download based on user's choice (0 = full, 1 = subset)\n",
    "    with requests.get(dump_url, stream=True) as r, open(dump_file_path, 'wb') as f:\n",
    "        r.raise_for_status()  # Raise an error for bad responses\n",
    "        \n",
    "        if choice == 1:  # Download 5 MB subset\n",
    "            downloaded_size = 0\n",
    "            for chunk in r.iter_content(1024):  # Download data in chunks of 1 KB\n",
    "                if chunk:\n",
    "                    f.write(chunk)  # Write the chunk to the file\n",
    "                    downloaded_size += len(chunk)  # Increase size counter\n",
    "                    if downloaded_size >= 5_000_000:  # Stop after 5 MB\n",
    "                        break\n",
    "        elif choice == 0:  # Download the full dump\n",
    "            for chunk in r.iter_content(1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded full dump.\")\n",
    "    \n",
    "    print(f\"Dump saved to {dump_file_path}\")\n",
    "    return dump_file_path\n",
    "\n",
    "user_choice = int(input(\"Enter 1 to download a 5MB subset or 0 for the full dump: \"))\n",
    "dump_url = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2'\n",
    "dump = download_wikipedia(dump_url, user_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting page extraction from wikipedia_dump_file.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/bin/wikiextractor\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wikiextractor/WikiExtractor.py\", line 640, in main\n",
      "    process_dump(input_file, args.templates, output_path, file_size,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wikiextractor/WikiExtractor.py\", line 393, in process_dump\n",
      "    for line in input:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/bz2.py\", line 176, in read1\n",
      "    return self._buffer.read1(size)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/_compression.py\", line 99, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction completed.  Extracted files are saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#Test\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mextract_wikipedia_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_subset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mextract_wikipedia_dump\u001b[0;34m(dump_file_path, output_dir, is_subset)\u001b[0m\n\u001b[1;32m     23\u001b[0m     extractor_command\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5M\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m#5  MB\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#  Run wikiextractor via subprocess\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextractor_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction completed.  Extracted files are saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    505\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1141\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1204\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1938\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1938\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1896\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1896\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1898\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def extract_wikipedia_dump(dump_file_path, output_dir='wikipedia_extracted', is_subset=True):\n",
    "    \"\"\"\n",
    "    Extracts plain text from the Wikipedia dump using WikiExtractor.\n",
    "\n",
    "    Parameters:\n",
    "    dump_file_path (str): The path to the downloaded Wikipedia dump file.\n",
    "    output_dir (str, optional): The directory where the extracted text will be saved.\n",
    "                                Defaults to 'wikipedia_extracted'.\n",
    "    is_subset (bool, optional): If True, assume the dump file is a small subset (e.g., 5MB).\n",
    "                                If False, process the full dump. Defaults to True.\n",
    "    \"\"\"\n",
    "    #  Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #  Use '--bytes' for smaller subsets\n",
    "    extractor_command = ['wikiextractor', '--json', '--no-templates', '-o', output_dir, dump_file_path]\n",
    "\n",
    "    if is_subset:\n",
    "        extractor_command.insert(1, '--bytes')\n",
    "        extractor_command.insert(2, '5M')  #5  MB\n",
    "\n",
    "    #  Run wikiextractor via subprocess\n",
    "    subprocess.run(extractor_command, check=True)\n",
    "\n",
    "    print(f\"Extraction completed.  Extracted files are saved in {output_dir}\")\n",
    "\n",
    "#Test\n",
    "extract_wikipedia_dump(dump, is_subset=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.43.4\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.43.4) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.43.4) (2024.2.2)\n",
      "Downloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed transformers-4.43.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.43.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Chunks:\n",
      "- \"\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Function to read all text content from JSON files in a folder\n",
    "def read_input_texts_from_folder(folder_path):\n",
    "    all_text = \"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Parse each line as JSON and extract the 'text' field\n",
    "                        data = json.loads(line)\n",
    "                        text_content = data.get('text', '').strip()  # Strip any leading/trailing whitespace\n",
    "                        if text_content:  # Ensure only non-empty content is added\n",
    "                            all_text += text_content + \"\\n\"\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip lines that are not valid JSON\n",
    "    return all_text\n",
    "\n",
    "# Folder where the Wikipedia data is extracted\n",
    "input_folder_path = 'wikipedia_extracted/AA'\n",
    "\n",
    "# Reading all the extracted text files from the folder\n",
    "input_text = read_input_texts_from_folder(input_folder_path)\n",
    "\n",
    "# Print the combined extracted text (for debugging purposes)\n",
    "# if input_text.strip():\n",
    "#     print(f\"Extracted Text: {input_text[:1000]}...\")  # Print first 1000 characters to check\n",
    "# else:\n",
    "#     print(\"No text extracted\")\n",
    "\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print(chunks)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
